package:
  name: tritonserver-backend-tensorrtllm-24.04
  version: 0.9.0
  epoch: 2
  description: NVIDIA Triton Inference Server TensorRTLLM Backend
  copyright:
    - license: Apache-2.0
  target-architecture:
    - x86_64
  resources:
    cpu: 64
    memory: 192Gi
  dependencies:
    runtime:
      - libgomp
      - openmp-17-dev
      - openmpi-cuda-${{vars.cuda-version}} # libnvinfer_plugin_tensorrt_llm.so links to its libmpi.so, not autodetected
      - cuda-toolkit-${{vars.cuda-version}}
      - nvidia-libcuda-${{vars.cuda-version}} # libnvinfer_plugin_tensorrt_llm.so links to libcuda.so, not autodetected
      - nvidia-nccl-cuda-${{vars.cuda-version}}
      - nvidia-cuda-cudart-${{vars.cuda-version}}
      - nvidia-cuda-cupti-${{vars.cuda-version}} # libtriton_tensorrtllm.so links to libcupti.so, not autodetected
      - nvidia-cudnn-8-cuda-12 # tensorrt_bindings/tensorrt.so uses cudnn-8
      - py3.10-torch-${{vars.torch-version}}-cuda-${{vars.cuda-version}}
      - numactl
      - tensorrt-${{vars.tensorrt-major-minor-version}}
      - tritonserver-2.45 # libtriton_tensorrtllm.so links to libtritonserver.so, not autodetected
      - c-ares
      # Manually enumerated python dependencies
      - py3.10-aiohttp
      - py3.10-dill
      - py3.10-huggingface-hub
      - py3.10-mpi4py
      - py3.10-numpy
      - py3.10-onnx
      - py3.10-pandas
      - py3.10-psutil
      - py3.10-pyarrow
      - py3.10-pybind11
      - py3.10-regex
      - py3.10-sentencepiece
  options:
    no-depends: true

environment:
  contents:
    packages:
      - bash
      - build-base
      - busybox
      - ca-certificates-bundle
      - cmake
      - file
      - gcc-12
      - gcc-12-default
      - git-lfs
      - grep
      - hwloc-cuda-${{vars.cuda-version}}-dev
      - libarchive-dev
      - ninja
      - nvidia-cuda-cccl-${{vars.cuda-version}}
      - nvidia-cuda-compat-${{vars.cuda-version}}
      - nvidia-cuda-cudart-${{vars.cuda-version}}
      - nvidia-cuda-cudart-${{vars.cuda-version}}-dev
      - nvidia-cuda-cuobjdump-${{vars.cuda-version}}
      - nvidia-cuda-cupti-${{vars.cuda-version}}
      - nvidia-cuda-cupti-${{vars.cuda-version}}-dev
      - nvidia-cuda-cuxxfilt-${{vars.cuda-version}}
      - nvidia-cuda-nvcc-${{vars.cuda-version}}
      - nvidia-cuda-nvcc-${{vars.cuda-version}}-dev
      - nvidia-cuda-nvml-dev-${{vars.cuda-version}}
      - nvidia-cuda-nvprof-${{vars.cuda-version}}
      - nvidia-cuda-nvprof-${{vars.cuda-version}}-dev
      - nvidia-cuda-nvrtc-${{vars.cuda-version}}
      - nvidia-cuda-nvrtc-${{vars.cuda-version}}-dev
      - nvidia-cuda-nvtx-${{vars.cuda-version}}
      - nvidia-cuda-nvtx-${{vars.cuda-version}}-dev
      - nvidia-cuda-profiler-api-${{vars.cuda-version}}
      - nvidia-cudnn-${{vars.cudnn-major-version}}-cuda-${{vars.cuda-major-version}}-dev
      - nvidia-driver
      - nvidia-driver-userspace
      - nvidia-libcublas-${{vars.cuda-version}}
      - nvidia-libcublas-${{vars.cuda-version}}-dev
      - nvidia-libcufft-${{vars.cuda-version}}
      - nvidia-libcufft-${{vars.cuda-version}}-dev
      - nvidia-libcufile-${{vars.cuda-version}}
      - nvidia-libcufile-${{vars.cuda-version}}-dev
      - nvidia-libcurand-${{vars.cuda-version}}
      - nvidia-libcurand-${{vars.cuda-version}}-dev
      - nvidia-libcusolver-${{vars.cuda-version}}
      - nvidia-libcusolver-${{vars.cuda-version}}-dev
      - nvidia-libcusparse-${{vars.cuda-version}}
      - nvidia-libcusparse-${{vars.cuda-version}}-dev
      - nvidia-libnvjitlink-${{vars.cuda-version}}
      - nvidia-libnvjitlink-${{vars.cuda-version}}-dev
      - nvidia-nccl-cuda-${{vars.cuda-version}}
      - openmpi-cuda-${{vars.cuda-version}}
      - openmpi-cuda-${{vars.cuda-version}}-dev
      - openssl-dev
      - py3.10-attrs
      - py3.10-build
      - py3.10-huggingface-hub
      - py3.10-importlib-metadata
      - py3.10-mpi4py
      - py3.10-numpy
      - py3.10-pillow
      - py3.10-pip
      - py3.10-protobuf
      - py3.10-pybind11
      - py3.10-setuptools
      - py3.10-torch-${{vars.torch-version}}-cuda-${{vars.cuda-version}}
      - py3.10-typing-extensions
      - py3.10-wheel
      - python-3.10
      - python-3.10-dev
      - rapidjson-dev
      - tensorrt-${{vars.tensorrt-major-minor-version}}
      - tensorrt-${{vars.tensorrt-major-minor-version}}-dev
      - uv
      - wget
      - zlib-dev
      # Manually enumerated python dependencies
      - py3.10-aiohttp
      - py3.10-dill
      - py3.10-onnx
      - py3.10-pandas
      - py3.10-psutil
      - py3.10-pyarrow
      - py3.10-regex
      - py3.10-sentencepiece

vars:
  cuda-version: 12.4
  cudnn-major-version: 8
  import: tensorrt_llm
  tensorrt-version: 9.3.0.1
  torch-version: 2.3

var-transforms:
  - from: ${{vars.cuda-version}}
    match: (\d+)\.\d+
    replace: $1
    to: cuda-major-version
  - from: ${{vars.tensorrt-version}}
    match: (\d+)(\.\d+)\.\d+\.\d+
    replace: $1$2
    to: tensorrt-major-minor-version

pipeline:
  - uses: git-checkout
    with:
      expected-commit: 76464e9be06600f3979acad9c14857938a66ff9f
      repository: https://github.com/triton-inference-server/tensorrtllm_backend.git
      tag: v${{package.version}}
      cherry-picks: |
        r24.04/99bdff055a88399cdb06c3509b9fca63f336c505: Update TensorRT-LLM backend (#427)
      destination: backend
      recurse-submodules: true

  - runs: |
      export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/tensorrt-${{vars.tensorrt-version}}/lib"
      export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cuda-${{vars.cuda-version}}/lib:/usr/local/cuda-${{vars.cuda-version}}/lib64:/usr/lib:/usr/local/cuda-${{vars.cuda-version}}/compat/"
      export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cudnn-${{vars.cudnn-major-version}}/lib64"
      export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/opt/tensorrt_llm/libs:/opt/tritonserver/lib"
      export PATH="$PATH:/usr/local/cuda-${{vars.cuda-version}}/bin:/usr/local/tensorrt-${{vars.tensorrt-version}}/bin"
      export SRCDIR=/home/build/backend
      export BUILDDIR=${SRCDIR}/build

      # Build a version of tensorrtllm for the cuda version for this version of tritonserver
      # Setup the virtualenv for tensortllm to get correct interpreters
      uv venv -p python3.10 --link-mode copy --system-site-packages /opt/tritonserver/venv
      source /opt/tritonserver/venv/bin/activate

      # The tensorrtllm_backend repository pulls in the tensorrt_llm repository as a submodule.
      # The tensorrt_llm repository uses git lfs to store the precompiled versions of the batch manager
      # https://github.com/NVIDIA/TensorRT-LLM/tree/a96cccafcf6365c128f004f779160951f8c0801c/cpp/tensorrt_llm
      # So we need to pull in the lfs files.
      cd ${SRCDIR}/tensorrt_llm
      git lfs install
      git lfs pull

      mkdir -p "${BUILDDIR}"
      cd ${BUILDDIR}

      export ARCH=x86_64
      cd ${BUILDDIR}

      # Here's how do determine the TRT VERSION
      # 1. Look at the requirements.txt file in TensorRT_LLM https://github.com/NVIDIA/TensorRT-LLM/blob/be9cd719f7d3149a5267c854284d83f717da0771/requirements.txt
      # 2. This will be installing a pip file for TensorRT that is the version you want to use for the given version of TensorRT_LLM
      # 3. The APK for TensorRT should be built at the same version as that pip
      # The TRT version should be in the tar.gz of the TensorRT distribution you download.
      export TRT_VERSION=${{vars.tensorrt-version}}
      export TRT_ROOT=/usr/local/tensorrt-${{vars.tensorrt-version}}

      # If we don't specify the location of CUDA_TOOLKIT_ROOT_DIR it won't be found
      # Setting BUILD_TESTS and BUILD_BENCHMARKS didn't fix the error

      # TODO(https://github.com/chainguard-dev/extra-packages/issues/902)
      sed -i 's/^torch/#torch/' ${SRCDIR}/tensorrt_llm/requirements.txt

      # Activate the virtual environment used by torch
      # Note that we add the USE_CXX11_ABI option because of
      # https://github.com/chainguard-dev/extra-packages/issues/518#issuecomment-2274442194
      echo "Executing in ${pwd}"
      python3.10 ../tensorrt_llm/scripts/build_wheel.py --trt_root ${TRT_ROOT} \
        -D "CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-${{vars.cuda-version}}/" -D "ENABLE_MULTI_DEVICE=1" \
        -D "USE_CXX11_ABI=1"

      # Install the wheel
      pip install --verbose \
        --no-index --no-build-isolation --no-deps \
        --force-reinstall --no-compile --prefix="/opt/tritonserver/venv" --root=${{targets.contextdir}} \
        ${SRCDIR}/tensorrt_llm/build/tensorrt_llm-*.whl

      requirements="${SRCDIR}/tensorrt_llm/requirements.txt"
      requirements_new="requirements.txt.new"
      cp "$requirements" "$requirements_new"
      apk_installed="build mpi4py numpy onnx psutil pandas sentencepiece setuptools wheel"
      for dep in $apk_installed; do
        sed -i "/^${dep}$/d" "$requirements_new"
      done
      # Dependencies discovered by iterating on `import tensorrt_llm'
      grep '^tensorrt==' "$requirements" | sed 's/^tensorrt/tensorrt_bindings/' >> "$requirements_new"
      manual_deps="datasets multiprocess pynvml<12 safetensors tokenizers>=0.14,<0.19 transformers xxhash"
      for dep in $manual_deps; do
        echo $dep >> "$requirements_new"
      done

      pip install --verbose \
        --requirement="$requirements_new" --force-reinstall --no-deps \
        --prefix=/opt/tritonserver/venv --root="${{targets.contextdir}}"
      python -m compileall --invalidation-mode=unchecked-hash -r100 ${{targets.contextdir}}/opt/tritonserver/venv

      # Don't be fooled the directory is inflight_batcher_llm but this is really building the
      # library for the triton trt llm backend.
      cmake "-DTRT_VERSION=${TRT_VERSION}" \
        "-DCMAKE_TOOLCHAIN_FILE=${CMAKE_TOOLCHAIN_FILE}" \
        "-DVCPKG_TARGET_TRIPLET=${VCPKG_TARGET_TRIPLET}" \
        "-DTRT_LIB_DIR=${TRT_ROOT}/targets/${ARCH}-linux-gnu/lib" \
        "-DTRT_INCLUDE_DIR=${TRT_ROOT}/include" \
        "-DUSE_CXX11_ABI:BOOL=ON" \
        "-DCMAKE_BUILD_TYPE=Release" \
        "-DCMAKE_INSTALL_PREFIX:PATH=/tmp/tritonbuild/tensorrtllm/install" \
        "-DTRITON_REPO_ORGANIZATION:STRING=https://github.com/triton-inference-server" \
        "-DTRITON_COMMON_REPO_TAG:STRING=r24.05" \
        "-DTRITON_CORE_REPO_TAG:STRING=r24.05" \
        "-DTRITON_BACKEND_REPO_TAG:STRING=r24.05" \
        "-DTRITON_ENABLE_GPU:BOOL=ON" \
        "-DTRITON_ENABLE_MALI_GPU:BOOL=OFF" \
        "-DTRITON_ENABLE_STATS:BOOL=ON" \
        "-DTRITON_ENABLE_METRICS:BOOL=ON" \
        "-DTRITON_ENABLE_MEMORY_TRACKER:BOOL=ON" \
        -S \
        ../inflight_batcher_llm \
        -B \
        .

      cmake --build . --config Release -j$(nproc)  -t install

      # Dest dir is the directory in final output where we should put the artifacts
      # See https://github.com/triton-inference-server/server/issues/7410#issuecomment-2218856792
      # Using a directory other than the default seems to cause problems at least for the python backend.
      export DESTDIR="${{targets.destdir}}/opt/tritonserver"

      # Create a destination directory for TensorRT
      export TRTLLMDEST="${{targets.destdir}}/opt/tensorrt_llm"

      export BACKENDDIR=${DESTDIR}/backends/tensorrtllm
      mkdir -p ${BACKENDDIR}

      # Print out a list of any so files in the build directory
      # We do this to help debug if we end up dropping a necessary artifact
      cd /home/build
      find ./ -name '*.so*'

      mkdir -p ${BACKENDDIR}/plugins

      mkdir -p ${TRTLLMDEST}/libs
      # ************************************************************************************************
      # Copy TensorRT-LLM artifacts
      #
      # These are the libs that get built here
      # https://github.com/NVIDIA/TensorRT-LLM/blob/250d9c293d5edbc2a45c20775b3150b1eb68b364/scripts/build_wheel.py#L190
      cp ${SRCDIR}/tensorrt_llm/cpp/build/tensorrt_llm/*.so ${TRTLLMDEST}/libs
      cp ${SRCDIR}/tensorrt_llm/cpp/build/tensorrt_llm/thop/*.so ${TRTLLMDEST}/libs

      cp -a ${SRCDIR}/tensorrt_llm/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin_tensorrt_llm.so* ${TRTLLMDEST}/libs

      # ************************************************
      # Copy LibTriton artifacts
      #*************************************************
      cp ${BUILDDIR}/libtriton*.so ${BACKENDDIR}
      deactivate

update:
  enabled: true
  github:
    identifier: triton-inference-server/tensorrtllm_backend
    strip-prefix: v
    use-tag: true
    tag-filter-prefix: v0.9.

test:
  environment:
    contents:
      packages:
        - posix-libc-utils # for ldd
    environment:
      LD_LIBRARY_PATH: /usr/local/tensorrt-${{vars.tensorrt-version}}/lib:/usr/local/cuda-${{vars.cuda-version}}/lib:/usr/local/cuda-${{vars.cuda-version}}/lib64::/usr/local/cuda-${{vars.cuda-version}}/compat:/usr/local/cudnn-${{vars.cudnn-major-version}}/lib64:/usr/local/cudnn-${{vars.cudnn-major-version}}/lib64:/opt/tensorrt_llm/libs:/opt/tritonserver/lib
  pipeline:
    - name: Verify tensorrtllm installation
      uses: python/import
      with:
        python: /opt/tritonserver/venv/bin/python3.10
        import: ${{vars.import}}
    - name: Scan .so files for undetected runtime dependencies
      runs: |
        ret=0
        apk info -L "${{package.name}}" | grep \.so$ | sed 's,^,/,' | while read obj; do
          echo -n "Scanning $obj... "
          if ldd "$obj" | grep "not found"; then
            echo "failed"
            ret=1
            continue
          fi
          echo "ok"
        done
        exit $ret
