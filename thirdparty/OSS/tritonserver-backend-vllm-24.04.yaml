#nolint:git-checkout-must-use-github-updates,valid-pipeline-git-checkout-tag
package:
  name: tritonserver-backend-vllm-24.04
  version: 24.04
  epoch: 2
  description: NVIDIA Triton Inference Server with vllm backend
  copyright:
    - license: BSD-3-Clause
  target-architecture:
    - x86_64
  resources:
    cpu: 16
    memory: 16Gi
  dependencies:
    runtime:
      - python-${{vars.py-version}}-base
      - cuda-toolkit-${{vars.cuda-version}}
      - nvidia-libcuda-${{vars.cuda-version}}
      - py${{vars.py-version}}-aiohttp
      - py${{vars.py-version}}-cloudpickle
      - py${{vars.py-version}}-filelock
      - py${{vars.py-version}}-huggingface-hub
      - py${{vars.py-version}}-importlib-metadata
      - py${{vars.py-version}}-jsonschema
      - py${{vars.py-version}}-numpy
      - py${{vars.py-version}}-openai
      - py${{vars.py-version}}-pandas
      - py${{vars.py-version}}-pillow
      - py${{vars.py-version}}-prometheus-client
      - py${{vars.py-version}}-protobuf
      - py${{vars.py-version}}-psutil
      - py${{vars.py-version}}-pydantic
      - py${{vars.py-version}}-pyyaml
      - py${{vars.py-version}}-pyzmq
      - py${{vars.py-version}}-regex
      - py${{vars.py-version}}-requests
      - py${{vars.py-version}}-sentencepiece
      - py${{vars.py-version}}-setuptools
      - py${{vars.py-version}}-six
      - py${{vars.py-version}}-sympy
      - py${{vars.py-version}}-torch-2.4-cuda-${{vars.cuda-version}}
      - py${{vars.py-version}}-torchvision-0.19-cuda-${{vars.cuda-version}}
      - py${{vars.py-version}}-tqdm
      - py${{vars.py-version}}-typing-extensions
      - py${{vars.py-version}}-xformers
      - python-${{vars.py-version}}
      - tritonserver-2.45
  options:
    no-depends: true

vars:
  cuda-version: 12.4
  cudnn-version: 9
  py-version: 3.10
  tensorrt-version: 8.6.3
  triton-repo-tag: r24.04
  vllm-version: 0.6.4
  torch-version: 2.4

var-transforms:
  - from: ${{vars.cuda-version}}
    match: '(\d+)\.\d+$'
    replace: '$1'
    to: cuda-major-version

environment:
  contents:
    packages:
      - build-base
      - busybox
      - ca-certificates-bundle
      - cmake
      - cuda-toolkit-${{vars.cuda-version}}-dev
      - gcc-11-default
      - gperftools-dev
      - ninja-build
      - numactl-dev
      - nvidia-cuda-cccl-${{vars.cuda-version}}
      - nvidia-cuda-profiler-api-${{vars.cuda-version}}
      - nvidia-cudnn-9-cuda-${{vars.cuda-major-version}}-dev
      - nvidia-driver
      - nvidia-libnvidia-ml-${{vars.cuda-version}}
      - nvidia-nccl-cuda-${{vars.cuda-version}}-dev
      - py${{vars.py-version}}-aiohttp
      - py${{vars.py-version}}-build
      - py${{vars.py-version}}-filelock
      - py${{vars.py-version}}-huggingface-hub
      - py${{vars.py-version}}-importlib-metadata
      - py${{vars.py-version}}-installer
      - py${{vars.py-version}}-jinja2
      - py${{vars.py-version}}-numpy
      - py${{vars.py-version}}-openai
      - py${{vars.py-version}}-pandas
      - py${{vars.py-version}}-pillow
      - py${{vars.py-version}}-pip
      - py${{vars.py-version}}-prometheus-client
      - py${{vars.py-version}}-protobuf
      - py${{vars.py-version}}-psutil
      - py${{vars.py-version}}-pydantic
      - py${{vars.py-version}}-pyyaml
      - py${{vars.py-version}}-pyzmq
      - py${{vars.py-version}}-requests
      - py${{vars.py-version}}-sentencepiece
      - py${{vars.py-version}}-setuptools
      - py${{vars.py-version}}-setuptools-scm
      - py${{vars.py-version}}-six
      - py${{vars.py-version}}-sympy
      - py${{vars.py-version}}-torch-${{vars.torch-version}}-cuda-${{vars.cuda-version}}
      - py${{vars.py-version}}-tqdm
      - py${{vars.py-version}}-typing-extensions
      - py${{vars.py-version}}-wheel
      - python-${{vars.py-version}}-dev
      - samurai
      - sccache
      - tcmalloc-minimal
      - uv
      - wolfi-base

pipeline:
  - uses: git-checkout
    working-directory: /home/build/vllm_backend
    with:
      # Becareful about changing the commit because you need to keep it in sync with the tags for the other repositories it pulls in.
      # N.B. r24.04 is a branch; there doesn't appear to be specific tags. So I think potentially they could push a change to the
      # branch and it would end up changing the commit we build but hopefully that's caught by expected commit
      expected-commit: a01475157290bdf6fd0f50688f69aafea41b04c5
      repository: https://github.com/triton-inference-server/vllm_backend.git
      branch: ${{vars.triton-repo-tag}}
      recurse-submodules: true

  - working-directory: /home/build/vllm_backend
    runs: |
      mkdir -p ${{targets.destdir}}/opt/tritonserver/backends/vllm
      cp -r src/* ${{targets.destdir}}/opt/tritonserver/backends/vllm

  - working-directory: /home/build/vllm
    pipeline:
      - uses: git-checkout
        working-directory: vllm
        with:
          repository: https://github.com/vllm-project/vllm.git
          tag: v${{vars.vllm-version}}
          expected-commit: 02dbf30e9a4389b41d95dd595bfe1224592dd404
      - uses: patch
        with:
          patches: ../vllm-remove-cmake.patch
      - runs: |
          # Build a version of vllm for the cuda version for this version of tritonserver
          # Setup the virtualenv for vllm to get correct interpreters
          uv venv -p python${{vars.py-version}} --link-mode copy --system-site-packages /opt/tritonserver/venv
          source /opt/tritonserver/venv/bin/activate

          export PATH=/usr/local/cuda-${{vars.cuda-version}}/bin:$PATH
          export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cudnn-9/lib64::$LD_LIBRARY_PATH
          export TORCH_CUDA_ARCH_LIST='7.0 7.5 8.0 8.6 8.9 9.0+PTX'
          export MAX_JOBS=16

          python${{vars.py-version}} use_existing_torch.py
          python${{vars.py-version}} -m build -n
          deactivate
      - runs: |
          # Install to venv destination
          source /opt/tritonserver/venv/bin/activate
          mkdir -p ${{targets.destdir}}/opt/tritonserver/venv

          pip install --verbose \
            --no-index --no-build-isolation --no-deps \
            --force-reinstall --no-compile --prefix="/opt/tritonserver/venv" --root=${{targets.contextdir}} dist/vllm-*.whl
          pip install --verbose \
            --requirement=requirements-cuda.txt --force-reinstall --no-deps \
            --prefix=/opt/tritonserver/venv --root="${{targets.contextdir}}"
          # Addt'l deps found by iterating on the `python -c "import vllm"` test
          pip install --verbose --no-deps --force-reinstall \
            --prefix=/opt/tritonserver/venv --root="${{targets.contextdir}}" \
            safetensors triton

          python -m compileall --invalidation-mode=unchecked-hash -r100 ${{targets.contextdir}}/opt/tritonserver/venv
          deactivate

test:
  pipeline:
    - name: Verify vllm installation
      runs: |
        export PATH=/opt/tritonserver/venv/bin:$PATH
        python -c "import vllm"

update:
  enabled: false
  exclude-reason: "Old versions on branches, new on tags no consistent upstream process"
