package:
  name: tritonserver-backend-tensorrtllm
  version: 0.12.0
  epoch: 0
  description: NVIDIA Triton Inference Server TensorRTLLM Backend
  copyright:
    - license: Apache-2.0
  target-architecture:
    - x86_64
  resources:
    cpu: 64
    memory: 192Gi
  dependencies:
    runtime:
      - libgomp
      - openmp-17-dev
      - openmpi-cuda-${{vars.cuda-version}} # libnvinfer_plugin_tensorrt_llm.so links to its libmpi.so, not autodetected
      - cuda-toolkit-${{vars.cuda-version}}
      - nvidia-libcuda-${{vars.cuda-version}} # libnvinfer_plugin_tensorrt_llm.so links to libcuda.so, not autodetected
      - nvidia-nccl-cuda-${{vars.cuda-version}}
      - nvidia-cuda-cudart-${{vars.cuda-version}}
      - nvidia-cuda-cupti-${{vars.cuda-version}} # libtriton_tensorrtllm.so links to libcupti.so, not autodetected
      - py3.10-torch-2.4-cuda-${{vars.cuda-version}}
      - numactl
      - tensorrt-${{vars.tensorrt-major-minor-version}}
      - tritonserver # libtriton_tensorrtllm.so links to libtritonserver.so, not autodetected
      - c-ares
  options:
    no-depends: true

environment:
  contents:
    packages:
      - bash
      - build-base
      - busybox
      - ca-certificates-bundle
      - cmake
      - file
      - gcc-12
      - gcc-12-default
      - git-lfs
      - hwloc-cuda-${{vars.cuda-version}}-dev
      - libarchive-dev
      - ninja
      - nvidia-cuda-cccl-${{vars.cuda-version}}
      - nvidia-cuda-compat-${{vars.cuda-version}}
      - nvidia-cuda-cudart-${{vars.cuda-version}}
      - nvidia-cuda-cudart-${{vars.cuda-version}}-dev
      - nvidia-cuda-cuobjdump-${{vars.cuda-version}}
      - nvidia-cuda-cupti-${{vars.cuda-version}}
      - nvidia-cuda-cupti-${{vars.cuda-version}}-dev
      - nvidia-cuda-cuxxfilt-${{vars.cuda-version}}
      - nvidia-cuda-nvcc-${{vars.cuda-version}}
      - nvidia-cuda-nvcc-${{vars.cuda-version}}-dev
      - nvidia-cuda-nvml-dev-${{vars.cuda-version}}
      - nvidia-cuda-nvprof-${{vars.cuda-version}}
      - nvidia-cuda-nvprof-${{vars.cuda-version}}-dev
      - nvidia-cuda-nvrtc-${{vars.cuda-version}}
      - nvidia-cuda-nvrtc-${{vars.cuda-version}}-dev
      - nvidia-cuda-nvtx-${{vars.cuda-version}}
      - nvidia-cuda-nvtx-${{vars.cuda-version}}-dev
      - nvidia-cuda-profiler-api-${{vars.cuda-version}}
      - nvidia-cudnn-${{vars.cudnn-major-version}}-cuda-${{vars.cuda-major-version}}-dev
      - nvidia-driver
      - nvidia-driver-userspace
      - nvidia-libcublas-${{vars.cuda-version}}
      - nvidia-libcublas-${{vars.cuda-version}}-dev
      - nvidia-libcufft-${{vars.cuda-version}}
      - nvidia-libcufft-${{vars.cuda-version}}-dev
      - nvidia-libcufile-${{vars.cuda-version}}
      - nvidia-libcufile-${{vars.cuda-version}}-dev
      - nvidia-libcurand-${{vars.cuda-version}}
      - nvidia-libcurand-${{vars.cuda-version}}-dev
      - nvidia-libcusolver-${{vars.cuda-version}}
      - nvidia-libcusolver-${{vars.cuda-version}}-dev
      - nvidia-libcusparse-${{vars.cuda-version}}
      - nvidia-libcusparse-${{vars.cuda-version}}-dev
      - nvidia-libnvjitlink-${{vars.cuda-version}}
      - nvidia-libnvjitlink-${{vars.cuda-version}}-dev
      - nvidia-nccl-cuda-${{vars.cuda-version}}
      - openmpi-cuda-${{vars.cuda-version}}
      - openmpi-cuda-${{vars.cuda-version}}-dev
      - openssl-dev
      - py3.10-attrs
      - py3.10-build
      - py3.10-huggingface-hub
      - py3.10-importlib-metadata
      - py3.10-mpi4py
      - py3.10-numpy
      - py3.10-pillow
      - py3.10-pip
      - py3.10-protobuf
      - py3.10-pybind11
      - py3.10-setuptools
      - py3.10-torch-2.4-cuda-${{vars.cuda-version}}
      - py3.10-typing-extensions
      - py3.10-wheel
      - python-3.10
      - python-3.10-dev
      - rapidjson-dev
      - tensorrt-${{vars.tensorrt-major-minor-version}}
      - tensorrt-${{vars.tensorrt-major-minor-version}}-dev
      - wget
      - zlib-dev

vars:
  cuda-version: 12.5
  cudnn-major-version: 9
  tensorrt-version: 10.3.0.26

var-transforms:
  - from: ${{vars.cuda-version}}
    match: (\d+)\.\d+
    replace: $1
    to: cuda-major-version
  - from: ${{vars.tensorrt-version}}
    match: (\d+)(\.\d+)\.\d+\.\d+
    replace: $1$2
    to: tensorrt-major-minor-version

pipeline:
  - uses: git-checkout
    with:
      expected-commit: 9a78477873574098c32467f0fd8a9d3b4c243d9b
      repository: https://github.com/triton-inference-server/tensorrtllm_backend.git
      tag: v${{package.version}}
      destination: backend
      recurse-submodules: true

  - runs: |
      export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/tensorrt-${{vars.tensorrt-version}}/lib"
      export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cuda-${{vars.cuda-version}}/lib:/usr/local/cuda-${{vars.cuda-version}}/lib64:/usr/lib:/usr/local/cuda-${{vars.cuda-version}}/compat/"
      export PATH="$PATH:/usr/local/cuda-${{vars.cuda-version}}/bin:/usr/local/tensorrt-${{vars.tensorrt-version}}/bin"
      export SRCDIR=/home/build/backend
      export BUILDDIR=${SRCDIR}/build

      # The tensorrtllm_backend repository pulls in the tensorrt_llm repository as a submodule.
      # The tensorrt_llm repository uses git lfs to store the precompiled versions of the batch manager
      # https://github.com/NVIDIA/TensorRT-LLM/tree/a96cccafcf6365c128f004f779160951f8c0801c/cpp/tensorrt_llm
      # So we need to pull in the lfs files.
      cd ${SRCDIR}/tensorrt_llm
      git lfs install
      git lfs pull

      mkdir -p "${BUILDDIR}"
      cd ${BUILDDIR}

      export ARCH=x86_64
      cd ${BUILDDIR}

      # Here's how do determine the TRT VERSION
      # 1. Look at the requirements.txt file in TensorRT_LLM https://github.com/NVIDIA/TensorRT-LLM/blob/be9cd719f7d3149a5267c854284d83f717da0771/requirements.txt
      # 2. This will be installing a pip file for TensorRT that is the version you want to use for the given version of TensorRT_LLM
      # 3. The APK for TensorRT should be built at the same version as that pip
      # The TRT version should be in the tar.gz of the TensorRT distribution you download.
      export TRT_VERSION=${{vars.tensorrt-version}}
      export TRT_ROOT=/usr/local/tensorrt-${{vars.tensorrt-version}}

      # If we don't specify the location of CUDA_TOOLKIT_ROOT_DIR it won't be found
      # Setting BUILD_TESTS and BUILD_BENCHMARKS didn't fix the error

      # TODO(https://github.com/chainguard-dev/extra-packages/issues/902)
      sed -i 's/^torch/#torch/' ${SRCDIR}/tensorrt_llm/requirements.txt

      # Activate the virtual environment used by torch
      # Note that we add the USE_CXX11_ABI option because of
      # https://github.com/chainguard-dev/extra-packages/issues/518#issuecomment-2274442194
      echo "Executing in ${pwd}"
      python3.10 ../tensorrt_llm/scripts/build_wheel.py --trt_root ${TRT_ROOT} \
        -D "CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-${{vars.cuda-version}}/" -D "ENABLE_MULTI_DEVICE=1" \
        -D "USE_CXX11_ABI=1"

      # Install the wheel
      pip install --prefix=${{targets.destdir}}/usr ${SRCDIR}/tensorrt_llm/build/tensorrt_llm-*.whl

      # Don't be fooled the directory is inflight_batcher_llm but this is really building the
      # library for the triton trt llm backend.
      cmake "-DTRT_VERSION=${TRT_VERSION}" \
        "-DCMAKE_TOOLCHAIN_FILE=${CMAKE_TOOLCHAIN_FILE}" \
        "-DVCPKG_TARGET_TRIPLET=${VCPKG_TARGET_TRIPLET}" \
        "-DTRT_LIB_DIR=${TRT_ROOT}/targets/${ARCH}-linux-gnu/lib" \
        "-DTRT_INCLUDE_DIR=${TRT_ROOT}/include" \
        "-DUSE_CXX11_ABI:BOOL=ON" \
        "-DCMAKE_BUILD_TYPE=Release" \
        "-DCMAKE_INSTALL_PREFIX:PATH=/tmp/tritonbuild/tensorrtllm/install" \
        "-DTRITON_REPO_ORGANIZATION:STRING=https://github.com/triton-inference-server" \
        "-DTRITON_COMMON_REPO_TAG:STRING=r24.05" \
        "-DTRITON_CORE_REPO_TAG:STRING=r24.05" \
        "-DTRITON_BACKEND_REPO_TAG:STRING=r24.05" \
        "-DTRITON_ENABLE_GPU:BOOL=ON" \
        "-DTRITON_ENABLE_MALI_GPU:BOOL=OFF" \
        "-DTRITON_ENABLE_STATS:BOOL=ON" \
        "-DTRITON_ENABLE_METRICS:BOOL=ON" \
        "-DTRITON_ENABLE_MEMORY_TRACKER:BOOL=ON" \
        -S \
        ../inflight_batcher_llm \
        -B \
        .

      cmake --build . --config Release -j20  -t install

      # Dest dir is the directory in final output where we should put the artifacts
      # See https://github.com/triton-inference-server/server/issues/7410#issuecomment-2218856792
      # Using a directory other than the default seems to cause problems at least for the python backend.
      export DESTDIR="${{targets.destdir}}/opt/tritonserver"

      # Create a destination directory for TensorRT
      export TRTLLMDEST="${{targets.destdir}}/opt/tensorrt_llm"

      export BACKENDDIR=${DESTDIR}/backends/tensorrtllm
      mkdir -p ${BACKENDDIR}

      # Print out a list of any so files in the build directory
      # We do this to help debug if we end up dropping a necessary artifact
      cd /home/build
      find ./ -name *.so

      mkdir -p ${BACKENDDIR}/plugins

      mkdir -p ${TRTLLMDEST}/libs
      # ************************************************************************************************
      # Copy TensorRT-LLM artifacts
      #
      # These are the libs that get built here
      # https://github.com/NVIDIA/TensorRT-LLM/blob/be9cd719f7d3149a5267c854284d83f717da0771/scripts/build_wheel.py#L214
      cp ${SRCDIR}/tensorrt_llm/cpp/build/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/nvrtcWrapper/libtensorrt_llm_nvrtc_wrapper.so \
        ${TRTLLMDEST}/libs

      cp ${SRCDIR}/tensorrt_llm/cpp/build/tensorrt_llm/*.so ${TRTLLMDEST}/libs

      cp ${SRCDIR}/tensorrt_llm/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin_tensorrt_llm.so ${TRTLLMDEST}/libs

      # ************************************************
      # Copy LibTriton artifacts
      #*************************************************
      cp ${BUILDDIR}/libtriton*.so ${BACKENDDIR}
      cp ${BUILDDIR}/trtllmExecutorWorker ${BACKENDDIR}

update:
  enabled: false

test:
  environment:
    contents:
      packages:
        - posix-libc-utils # for ldd
        - openmpi-cuda-${{vars.cuda-version}} # libnvinfer_plugin_tensorrt_llm.so links to its libmpi.so, not autodetected
        - nvidia-libcuda-${{vars.cuda-version}} # libnvinfer_plugin_tensorrt_llm.so links to libcuda.so, not autodetected
        - nvidia-cuda-cupti-${{vars.cuda-version}} # libtriton_tensorrtllm.so links to libcupti.so, not autodetected
        - tritonserver # libtriton_tensorrtllm.so links to libtritonserver.so, not autodetected
    environment:
      LD_LIBRARY_PATH: /usr/local/cuda-12.5/lib64:/usr/local/cudnn-9/lib64:/opt/tensorrt_llm/libs:/opt/tritonserver/lib
  pipeline:
    - name: Scan .so files for undetected runtime dependencies
      runs: |
        ret=0
        apk info -L "${{package.name}}" | grep \.so$ | sed 's,^,/,' | while read obj; do
          echo -n "Scanning $obj... "
          if ldd "$obj" | grep "not found"; then
            echo "failed"
            ret=1
            continue
          fi
          echo "ok"
        done
        exit $ret
