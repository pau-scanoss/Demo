package:
  name: tritonserver-backend-vllm
  version: 24.08
  epoch: 0
  description: NVIDIA Triton Inference Server with vllm backend
  copyright:
    - license: BSD-3-Clause
  target-architecture:
    - x86_64
  dependencies:
    runtime:
      - python-3.10-base
      - py3.10-vllm-cuda-${{vars.cuda-version}}
  options:
    no-depends: true

vars:
  cuda-version: 12.6

environment:
  contents:
    packages:
      - build-base
      - busybox
      - ca-certificates-bundle
      - wolfi-base

pipeline:
  - uses: git-checkout
    with:
      expected-commit: 98947a7c641057a1e3cf8fc9805dc4e3b93e7b66
      repository: https://github.com/triton-inference-server/vllm_backend.git
      tag: v${{package.version}}
      recurse-submodules: true

  - runs: |
      mkdir -p ${{targets.destdir}}/opt/tritonserver/backends/vllm
      cp -r ./src/* ${{targets.destdir}}/opt/tritonserver/backends/vllm

update:
  enabled: true
  github:
    identifier: triton-inference-server/vllm_backend
    strip-prefix: v
    use-tag: true
    tag-filter-prefix: v24.
